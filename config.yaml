model_config:
  vocab_size: 35000
  hidden_size: 256
  num_hidden_layers: 8
  num_attention_heads: 8
  intermediate_size: 1024
  max_position_embeddings: 512
  layer_norm_eps: 1e-5
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1

data_config:
  cm_samples: 30000
  synth_samples: 20000
  max_length: 512
  min_words: 50

training_config:
  batch_size: 32
  epochs: 15
  lr: 1e-3 # CHANGE FOR NEXT TEST 15/JAN
  log_interval: 10
  save_interval: 1
  seed: 42
